{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b06d19-8478-4df4-bb2d-99dfa4f07aad",
   "metadata": {
    "id": "a2b06d19-8478-4df4-bb2d-99dfa4f07aad"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1909f1-a5fa-4393-8213-a0c8bca6525d",
   "metadata": {
    "id": "1e1909f1-a5fa-4393-8213-a0c8bca6525d",
    "outputId": "ac5e0363-8a14-42c6-9409-fa9f660c1038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-pom added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d94d632f-f263-4c1e-b4fd-840adeb5cb41;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-pom;1.12.365 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 181ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-pom;1.12.365 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d94d632f-f263-4c1e-b4fd-840adeb5cb41\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/5ms)\n",
      "24/06/27 20:02:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "ACCESS_KEY = \"yuQTUA75jptY1N30aLfb\"\n",
    "SECRET_KEY = \"Lz6ajCsZydsIaHCOuAoCUc8VbK5G70nyQsHeIDpq\"\n",
    "MINIO_URL = \"http://minio:9000\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"HW2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", False) \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", True) \\\n",
    "    .config(\"spark.sql.legacy.allowNonEmptyLocationInCTAS\", True)\\\n",
    "    .config(\"spark.executor.memory\", \"600M\") \\\n",
    "    .config(\"spark.driver.memory\", \"600M\") \\\n",
    "    .config('spark.jars.packages',\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-pom:1.12.365,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\"\n",
    "    ) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_URL) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb74cf6d",
   "metadata": {
    "id": "fb74cf6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://79ddbe9fc153:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HW2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff80683910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9ece4-83c7-486a-84b6-192fca40d533",
   "metadata": {
    "id": "02f9ece4-83c7-486a-84b6-192fca40d533"
   },
   "source": [
    "# Задание 1\n",
    "\n",
    "## Входные данные\n",
    "- Файл с данными по оттоку телеком оператора в США (churn.csv)\n",
    "- Справочник с названиями штатов (state.json)\n",
    "- Справочник с численностью населения территорий (определяется полем area code) внутри штатов (state.json)\n",
    "- Террия с численностью населения меньше 10_000 считается **мелкой**\n",
    "\n",
    "## Что нужно сделать\n",
    "1. Посчитать количество отточных и неотточных абонентов (поле churn), исключив **мелкие** территории\n",
    "2. Отчет должен быть выполнен в разрезе **каждого штата** с его полным наименованием\n",
    "3. Описать возникающие узкие места при выполнении данной операции\n",
    "4. Применить один из способов оптимизации для ускорения выполнения запроса (при допущении, что справочник численности населения **сильно меньше** основных данных)\n",
    "5. Если существует еще какой-то способ, применить также и его отдельно от п.4 (при допущении, что справочник численности населения **сопоставим по размеру** с основными данными)\n",
    "6. Кратко описать реализованные способы и в чем их практическая польза\n",
    "\n",
    "- P.S. Одним из выбранных способов должен быть Bucket specific join\n",
    "- P.P.S. При обосновании предлагаем прикладывать запуска команды df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b58da6b-e2ff-4cd6-9995-c4d35e5c2745",
   "metadata": {
    "id": "9b58da6b-e2ff-4cd6-9995-c4d35e5c2745",
    "outputId": "b01a48d1-43eb-43e8-d781-ddeecb1c8452"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 20:02:55 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "churn_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/churn.csv\")\n",
    "state_dict = spark.read.json(\"s3a://input/data/state.json\")\n",
    "pop_dict = spark.read.json(\"s3a://input/data/population.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18763c6",
   "metadata": {
    "id": "f18763c6"
   },
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecaee4ae",
   "metadata": {
    "id": "ecaee4ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[id: string, state: string, account length: string, area code: string, phone number: string, international plan: string, voice mail plan: string, number vmail messages: string, total day minutes: string, total day calls: string, total day charge: string, total eve minutes: string, total eve calls: string, total eve charge: string, total night minutes: string, total night calls: string, total night charge: string, total intl minutes: string, total intl calls: string, total intl charge: string, customer service calls: string, churn: string, val: string],\n",
       " DataFrame[state_id: string, state_name: string],\n",
       " DataFrame[area code: bigint, population: bigint])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df, state_dict, pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0d8d7a-c17c-4e6f-acc0-2a95a0dc4690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_churn = churn_df.join(pop_dict, on=\"area code\", how=\"inner\") \\\n",
    "    .filter(F.col(\"population\") > 10000)\n",
    "\n",
    "result = filtered_churn.join(state_dict, filtered_churn.state==state_dict.state_id, how=\"left\") \\\n",
    "    .groupBy(\"state_name\") \\\n",
    "    .pivot(\"churn\") \\\n",
    "    .agg(F.count(\"*\")) \\\n",
    "    .orderBy(\"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14b64c8",
   "metadata": {
    "id": "a14b64c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(12) Sort [state_name#72 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(state_name#72 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=307]\n",
      "   +- *(11) Project [state_name#72, __pivot_count(1) AS `count(1)`#207[0] AS False#208L, __pivot_count(1) AS `count(1)`#207[1] AS True#209L]\n",
      "      +- HashAggregate(keys=[state_name#72], functions=[pivotfirst(churn#38, count(1)#201L, False, True, 0, 0)])\n",
      "         +- Exchange hashpartitioning(state_name#72, 200), ENSURE_REQUIREMENTS, [plan_id=302]\n",
      "            +- HashAggregate(keys=[state_name#72], functions=[partial_pivotfirst(churn#38, count(1)#201L, False, True, 0, 0)])\n",
      "               +- *(10) HashAggregate(keys=[state_name#72, churn#38], functions=[count(1)])\n",
      "                  +- Exchange hashpartitioning(state_name#72, churn#38, 200), ENSURE_REQUIREMENTS, [plan_id=297]\n",
      "                     +- *(9) HashAggregate(keys=[state_name#72, churn#38], functions=[partial_count(1)])\n",
      "                        +- *(9) Project [churn#38, state_name#72]\n",
      "                           +- *(9) SortMergeJoin [state#18], [state_id#71], LeftOuter\n",
      "                              :- *(6) Sort [state#18 ASC NULLS FIRST], false, 0\n",
      "                              :  +- Exchange hashpartitioning(state#18, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "                              :     +- *(5) Project [state#18, churn#38]\n",
      "                              :        +- *(5) SortMergeJoin [cast(area code#20 as bigint)], [area code#83L], Inner\n",
      "                              :           :- *(2) Sort [cast(area code#20 as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :           :  +- Exchange hashpartitioning(cast(area code#20 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=263]\n",
      "                              :           :     +- *(1) Filter isnotnull(area code#20)\n",
      "                              :           :        +- FileScan csv [state#18,area code#20,churn#38] Batched: false, DataFilters: [isnotnull(area code#20)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/churn.csv], PartitionFilters: [], PushedFilters: [IsNotNull(area code)], ReadSchema: struct<state:string,area code:string,churn:string>\n",
      "                              :           +- *(4) Sort [area code#83L ASC NULLS FIRST], false, 0\n",
      "                              :              +- Exchange hashpartitioning(area code#83L, 200), ENSURE_REQUIREMENTS, [plan_id=272]\n",
      "                              :                 +- *(3) Project [area code#83L]\n",
      "                              :                    +- *(3) Filter ((isnotnull(population#84L) AND (population#84L > 10000)) AND isnotnull(area code#83L))\n",
      "                              :                       +- FileScan json [area code#83L,population#84L] Batched: false, DataFilters: [isnotnull(population#84L), (population#84L > 10000), isnotnull(area code#83L)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/population.json], PartitionFilters: [], PushedFilters: [IsNotNull(population), GreaterThan(population,10000), IsNotNull(area code)], ReadSchema: struct<area code:bigint,population:bigint>\n",
      "                              +- *(8) Sort [state_id#71 ASC NULLS FIRST], false, 0\n",
      "                                 +- Exchange hashpartitioning(state_id#71, 200), ENSURE_REQUIREMENTS, [plan_id=288]\n",
      "                                    +- *(7) Filter isnotnull(state_id#71)\n",
      "                                       +- FileScan json [state_id#71,state_name#72] Batched: false, DataFilters: [isnotnull(state_id#71)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/state.json], PartitionFilters: [], PushedFilters: [IsNotNull(state_id)], ReadSchema: struct<state_id:string,state_name:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================>               (141 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+\n",
      "|state_name          |False|True|\n",
      "+--------------------+-----+----+\n",
      "|Alabama             |58   |7   |\n",
      "|Alaska              |44   |3   |\n",
      "|Arizona             |53   |4   |\n",
      "|Arkansas            |35   |9   |\n",
      "|California          |21   |8   |\n",
      "|Colorado            |49   |7   |\n",
      "|Connecticut         |56   |8   |\n",
      "|Delaware            |48   |8   |\n",
      "|District of Columbia|43   |4   |\n",
      "|Florida             |45   |8   |\n",
      "|Georgia             |42   |8   |\n",
      "|Hawaii              |42   |3   |\n",
      "|Idaho               |53   |9   |\n",
      "|Illinois            |49   |5   |\n",
      "|Indiana             |59   |9   |\n",
      "|Iowa                |34   |3   |\n",
      "|Kansas              |50   |12  |\n",
      "|Kentucky            |51   |8   |\n",
      "|Louisiana           |47   |4   |\n",
      "|Maine               |49   |13  |\n",
      "+--------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "result.explain()\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac360b-c6c4-4143-bace-7eb558edf277",
   "metadata": {},
   "source": [
    "**Узкие места**: \n",
    "1. Перекос данных в файлах state.json и population.json, поэтому некоторые рабочие узлы могут получать больше данных, что замедлит выполнение операций.\n",
    "2. Можно использовать broadcast has join, так как у нас небольшие файлы (< 10 мб)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ca16c",
   "metadata": {
    "id": "d58ca16c"
   },
   "source": [
    "### Оптимизация 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6208cd-6183-4c45-bcde-ec89fd47363e",
   "metadata": {},
   "source": [
    "Используем **broadcast has join**: с помощью него данные будут распределены локально на все рабочие узлы, также данные из обоих датасетов при соединении не нужно будет перетасовывать (так как будет перетасовываться только бОльший датасет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8db731b",
   "metadata": {
    "id": "b8db731b"
   },
   "outputs": [],
   "source": [
    "broadcasted_pop_dict = F.broadcast(pop_dict)\n",
    "broadcasted_state_dict = F.broadcast(state_dict)\n",
    "\n",
    "broadcasted_filtered_churn = churn_df.join(broadcasted_pop_dict, on=\"area code\", how=\"inner\") \\\n",
    "    .filter(F.col(\"population\") > 10000)\n",
    "\n",
    "result = broadcasted_filtered_churn.join(broadcasted_state_dict, broadcasted_filtered_churn.state==state_dict.state_id, how=\"left\") \\\n",
    "    .groupBy(\"state_name\") \\\n",
    "    .pivot(\"churn\") \\\n",
    "    .agg(F.count(\"*\")) \\\n",
    "    .orderBy(\"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cea922ac",
   "metadata": {
    "id": "cea922ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) Sort [state_name#72 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(state_name#72 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=826]\n",
      "   +- *(5) Project [state_name#72, __pivot_count(1) AS `count(1)`#380[0] AS False#381L, __pivot_count(1) AS `count(1)`#380[1] AS True#382L]\n",
      "      +- HashAggregate(keys=[state_name#72], functions=[pivotfirst(churn#38, count(1)#374L, False, True, 0, 0)])\n",
      "         +- Exchange hashpartitioning(state_name#72, 200), ENSURE_REQUIREMENTS, [plan_id=821]\n",
      "            +- HashAggregate(keys=[state_name#72], functions=[partial_pivotfirst(churn#38, count(1)#374L, False, True, 0, 0)])\n",
      "               +- *(4) HashAggregate(keys=[state_name#72, churn#38], functions=[count(1)])\n",
      "                  +- Exchange hashpartitioning(state_name#72, churn#38, 200), ENSURE_REQUIREMENTS, [plan_id=816]\n",
      "                     +- *(3) HashAggregate(keys=[state_name#72, churn#38], functions=[partial_count(1)])\n",
      "                        +- *(3) Project [churn#38, state_name#72]\n",
      "                           +- *(3) BroadcastHashJoin [state#18], [state_id#71], LeftOuter, BuildRight, false\n",
      "                              :- *(3) Project [state#18, churn#38]\n",
      "                              :  +- *(3) BroadcastHashJoin [cast(area code#20 as bigint)], [area code#83L], Inner, BuildRight, false\n",
      "                              :     :- *(3) Filter isnotnull(area code#20)\n",
      "                              :     :  +- FileScan csv [state#18,area code#20,churn#38] Batched: false, DataFilters: [isnotnull(area code#20)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/churn.csv], PartitionFilters: [], PushedFilters: [IsNotNull(area code)], ReadSchema: struct<state:string,area code:string,churn:string>\n",
      "                              :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=803]\n",
      "                              :        +- *(1) Project [area code#83L]\n",
      "                              :           +- *(1) Filter ((isnotnull(population#84L) AND (population#84L > 10000)) AND isnotnull(area code#83L))\n",
      "                              :              +- FileScan json [area code#83L,population#84L] Batched: false, DataFilters: [isnotnull(population#84L), (population#84L > 10000), isnotnull(area code#83L)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/population.json], PartitionFilters: [], PushedFilters: [IsNotNull(population), GreaterThan(population,10000), IsNotNull(area code)], ReadSchema: struct<area code:bigint,population:bigint>\n",
      "                              +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=810]\n",
      "                                 +- *(2) Filter isnotnull(state_id#71)\n",
      "                                    +- FileScan json [state_id#71,state_name#72] Batched: false, DataFilters: [isnotnull(state_id#71)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/state.json], PartitionFilters: [], PushedFilters: [IsNotNull(state_id)], ReadSchema: struct<state_id:string,state_name:string>\n",
      "\n",
      "\n",
      "+--------------------+-----+----+\n",
      "|state_name          |False|True|\n",
      "+--------------------+-----+----+\n",
      "|Alabama             |58   |7   |\n",
      "|Alaska              |44   |3   |\n",
      "|Arizona             |53   |4   |\n",
      "|Arkansas            |35   |9   |\n",
      "|California          |21   |8   |\n",
      "|Colorado            |49   |7   |\n",
      "|Connecticut         |56   |8   |\n",
      "|Delaware            |48   |8   |\n",
      "|District of Columbia|43   |4   |\n",
      "|Florida             |45   |8   |\n",
      "|Georgia             |42   |8   |\n",
      "|Hawaii              |42   |3   |\n",
      "|Idaho               |53   |9   |\n",
      "|Illinois            |49   |5   |\n",
      "|Indiana             |59   |9   |\n",
      "|Iowa                |34   |3   |\n",
      "|Kansas              |50   |12  |\n",
      "|Kentucky            |51   |8   |\n",
      "|Louisiana           |47   |4   |\n",
      "|Maine               |49   |13  |\n",
      "+--------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "result.explain()\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac80fd0",
   "metadata": {
    "id": "1ac80fd0"
   },
   "source": [
    "### Оптимизация 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf7eed-879e-4fa6-a66b-b7de4d899d7d",
   "metadata": {},
   "source": [
    "Используем **bucketBy**, чтобы избежать дополнительных операций сортировки и перетасовки при соединении датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a71375",
   "metadata": {
    "id": "92a71375"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "churn_df.repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(100, col=\"area code\") \\\n",
    "    .option(\"path\", \"s3a://input/data/buckets/churn_df_bucket\") \\\n",
    "    .saveAsTable(\"churn_df_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85298a74-7951-4865-b914-c97cbc97fd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pop_dict \\\n",
    "    .withColumn(\"area code\", F.col(\"area code\").cast(\"string\")) \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .bucketBy(100, \"area code\") \\\n",
    "    .option(\"path\", \"s3a://input/data/buckets/pop_dict_bucket\") \\\n",
    "    .saveAsTable(\"pop_dict_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f23bca-cb63-40b3-aa2b-12ba8cdcce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "churn_df_bucket = spark.table(\"churn_df_bucket\")\n",
    "pop_dict_bucket = spark.table(\"pop_dict_bucket\")\n",
    "\n",
    "filtered_buckets = churn_df_bucket.join(pop_dict_bucket, on=\"area code\", how=\"inner\") \\\n",
    "    .filter(F.col(\"population\") > 10000)\n",
    "\n",
    "result = filtered_buckets.join(state_dict, filtered_buckets.state==state_dict.state_id, how=\"left\") \\\n",
    "    .groupBy(\"state_name\") \\\n",
    "    .pivot(\"churn\") \\\n",
    "    .agg(F.count(\"*\")) \\\n",
    "    .orderBy(\"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c937295-20e4-4363-a69b-4e2383c6e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===============================================>       (87 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+\n",
      "|state_name          |False|True|\n",
      "+--------------------+-----+----+\n",
      "|Alabama             |58   |7   |\n",
      "|Alaska              |44   |3   |\n",
      "|Arizona             |53   |4   |\n",
      "|Arkansas            |35   |9   |\n",
      "|California          |21   |8   |\n",
      "|Colorado            |49   |7   |\n",
      "|Connecticut         |56   |8   |\n",
      "|Delaware            |48   |8   |\n",
      "|District of Columbia|43   |4   |\n",
      "|Florida             |45   |8   |\n",
      "|Georgia             |42   |8   |\n",
      "|Hawaii              |42   |3   |\n",
      "|Idaho               |53   |9   |\n",
      "|Illinois            |49   |5   |\n",
      "|Indiana             |59   |9   |\n",
      "|Iowa                |34   |3   |\n",
      "|Kansas              |50   |12  |\n",
      "|Kentucky            |51   |8   |\n",
      "|Louisiana           |47   |4   |\n",
      "|Maine               |49   |13  |\n",
      "+--------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f06364",
   "metadata": {
    "id": "40f06364"
   },
   "source": [
    "# Задание 2\n",
    "\n",
    "## Входные данные\n",
    "\n",
    "*skew_transactions.csv* - информация о длительности просомтра контента пользователям\n",
    "колонки:\n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "2. element_uid — уникальный идентификатор контента\n",
    "3. watched_time — время просмотра в секундах\n",
    "\n",
    "*catalogue.json* - каталог с описанием контента и метаинформации по нему\n",
    "колонки:\n",
    "1. type — тип элемента\n",
    "2. duration — длительность в минутах (средняя длительность эпизода в случае с сериалами и многосерийными фильмами), округлённая до десятков\n",
    "3. attributes — анонимизированные атрибуты данного элемента\n",
    "4. availability — доступные права на элемент(subscription, purchase, rent)\n",
    "5. feature_1 — анонимизированная вещественная переменная\n",
    "6. feature_2 — анонимизированная вещественная переменная\n",
    "7. feature_3 — анонимизированная порядковая переменная\n",
    "8. feature_4 — анонимизированная вещественная переменная\n",
    "9. feature_5 — анонимизированная вещественная переменная\n",
    "\n",
    "## Что нужно сделать\n",
    "1. Выполните join основных данных со справочником используя DataFrame API (по колонке id для контента - `element_uid`)\n",
    "2. Описать проблему в датасетах с точки зрения обработки Spark\n",
    "3. Решить задачу любым способом\n",
    "4. Решить задачу с помощью salt-join подхода\n",
    "\n",
    "P.S. Как вы можете заметить при просмотре данных по пользователями, нужный нам ключ для операции будет перекошен (90% строк представлены на фильм, очень популярный среди смотревших) - это нужно доказать в рамках п.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740055d",
   "metadata": {
    "id": "0740055d"
   },
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2e8c69",
   "metadata": {
    "id": "5e2e8c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde95359",
   "metadata": {
    "id": "dde95359"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31fef6e9-a3ca-4971-8c83-470bd76dec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_transactions_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/skew_transactions.csv\")\n",
    "catalogue_df = pd.read_json(\"catalogue.json\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fae7e6e4-663f-474e-ba6d-c4f2a154a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_df = catalogue_df.transpose().reset_index()\n",
    "catalogue_df = catalogue_df.rename(columns={\"index\": \"element_uid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "696dee54-609b-448b-bec1-239e56faa831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[element_uid: bigint, type: string, availability: string, duration: string, feature_1: string, feature_2: string, feature_3: string, feature_4: string, feature_5: string, attributes: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogue_df = spark.createDataFrame(catalogue_df)\n",
    "catalogue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec1b454-f9db-4256-8a86-7c815e27ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = skew_transactions_df.join(catalogue_df.select(\"element_uid\", \"type\", \"availability\", \"duration\"), \"element_uid\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df30c61d",
   "metadata": {
    "id": "df30c61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [element_uid#706, user_uid#707, watched_time#708, type#713, availability#714, duration#715]\n",
      "+- *(4) SortMergeJoin [cast(element_uid#706 as bigint)], [element_uid#712L], LeftOuter\n",
      "   :- *(1) Sort [cast(element_uid#706 as bigint) ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cast(element_uid#706 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=1510]\n",
      "   :     +- FileScan csv [element_uid#706,user_uid#707,watched_time#708] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/skew_transactions.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<element_uid:string,user_uid:string,watched_time:string>\n",
      "   +- *(3) Sort [element_uid#712L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(element_uid#712L, 200), ENSURE_REQUIREMENTS, [plan_id=1521]\n",
      "         +- *(2) Project [element_uid#712L, type#713, availability#714, duration#715]\n",
      "            +- *(2) Filter isnotnull(element_uid#712L)\n",
      "               +- *(2) Scan ExistingRDD[element_uid#712L,type#713,availability#714,duration#715,feature_1#716,feature_2#717,feature_3#718,feature_4#719,feature_5#720,attributes#721]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 20:06:49 WARN TaskSetManager: Stage 37 contains a task of very large size (1035 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------+-----+------------------------------------+--------+\n",
      "|element_uid|user_uid|watched_time|type |availability                        |duration|\n",
      "+-----------+--------+------------+-----+------------------------------------+--------+\n",
      "|6130       |563180  |3264        |movie|['purchase', 'rent']                |90      |\n",
      "|2714       |408794  |7583        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |247546  |4450        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |43556   |7569        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |169048  |7577        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |542158  |8834        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |356858  |7763        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |277533  |4781        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |345703  |7639        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |479550  |7710        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |590153  |7923        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |343066  |268         |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |57572   |7616        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |452625  |22107       |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |194183  |6430        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |537894  |7716        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |284981  |7586        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |65250   |7903        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |461130  |7934        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "|2714       |381351  |1238        |movie|['purchase', 'rent', 'subscription']|130     |\n",
      "+-----------+--------+------------+-----+------------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "result.explain()\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea6628-ed8d-4bcd-9d9e-36484df1c3dd",
   "metadata": {},
   "source": [
    "**Проблема**:\n",
    "Есть перекос в датасете skew_transactions.csv, в результате чего наиболее частые рабочие узлы будут перегружены, так как получат больше данных для обработки. Также перегруженные узлы могут привести к тому, что у нас не останется доступной памяти и придется использовать, например, дисковое пространство, что скажется на скорости выполнения операций. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c868f",
   "metadata": {
    "id": "108c868f"
   },
   "source": [
    "### Решение с оптимизацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c875e042",
   "metadata": {
    "id": "c875e042"
   },
   "outputs": [],
   "source": [
    "# Используем salt join \n",
    "skew_transactions_df = skew_transactions_df.withColumn(\"salt\", (F.rand() * 5).cast(\"int\"))\n",
    "catalogue_df= catalogue_df.withColumn(\"salt\", (F.rand() * 5).cast(\"int\"))\n",
    "\n",
    "skew_transactions_df = skew_transactions_df.repartition(5, \"element_uid\", \"salt\")\n",
    "catalogue_df = catalogue_df.repartition(5, \"element_uid\", \"salt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d29a56c-4bef-4fba-bd83-eb8f2e30b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = skew_transactions_df.join(catalogue_df.select(\"element_uid\", \"type\", \"availability\", \"duration\", \"salt\"), [\"element_uid\", \"salt\"], \"left\")\n",
    "grouped_result = result.groupBy(\"element_uid\", \"salt\").agg(F.count(\"*\").alias(\"count\"))\n",
    "result = grouped_result.select(\"element_uid\", \"salt\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07a6cecc",
   "metadata": {
    "id": "07a6cecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[element_uid#706, salt#771], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(element_uid#706, salt#771, 200), ENSURE_REQUIREMENTS, [plan_id=1680]\n",
      "   +- *(5) HashAggregate(keys=[element_uid#706, salt#771], functions=[partial_count(1)])\n",
      "      +- *(5) Project [element_uid#706, salt#771]\n",
      "         +- *(5) SortMergeJoin [cast(element_uid#706 as bigint), salt#771], [element_uid#712L, salt#776], LeftOuter\n",
      "            :- *(2) Sort [cast(element_uid#706 as bigint) ASC NULLS FIRST, salt#771 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(cast(element_uid#706 as bigint), salt#771, 200), ENSURE_REQUIREMENTS, [plan_id=1665]\n",
      "            :     +- Exchange hashpartitioning(element_uid#706, salt#771, 5), REPARTITION_BY_NUM, [plan_id=1664]\n",
      "            :        +- *(1) Project [element_uid#706, cast((rand(-3466626343010116864) * 5.0) as int) AS salt#771]\n",
      "            :           +- FileScan csv [element_uid#706] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/skew_transactions.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<element_uid:string>\n",
      "            +- *(4) Sort [element_uid#712L ASC NULLS FIRST, salt#776 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(element_uid#712L, salt#776, 200), REPARTITION_BY_NUM, [plan_id=1671]\n",
      "                  +- *(3) Filter (isnotnull(element_uid#712L) AND isnotnull(salt#776))\n",
      "                     +- *(3) Project [element_uid#712L, cast((rand(5027362053211843018) * 5.0) as int) AS salt#776]\n",
      "                        +- *(3) Scan ExistingRDD[element_uid#712L,type#713,availability#714,duration#715,feature_1#716,feature_2#717,feature_3#718,feature_4#719,feature_5#720,attributes#721]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 20:07:10 WARN TaskSetManager: Stage 50 contains a task of very large size (1035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+\n",
      "|element_uid|salt|count|\n",
      "+-----------+----+-----+\n",
      "|2692       |1   |57   |\n",
      "|7789       |2   |46   |\n",
      "|4886       |4   |71   |\n",
      "|3303       |3   |45   |\n",
      "|8408       |4   |46   |\n",
      "|2522       |4   |36   |\n",
      "|3023       |1   |37   |\n",
      "|5694       |2   |50   |\n",
      "|9971       |0   |60   |\n",
      "|6105       |1   |55   |\n",
      "|9876       |2   |54   |\n",
      "|9766       |4   |62   |\n",
      "|4401       |4   |57   |\n",
      "|4888       |0   |40   |\n",
      "|9794       |4   |38   |\n",
      "|9404       |1   |64   |\n",
      "|8968       |4   |32   |\n",
      "|9138       |0   |64   |\n",
      "|7252       |4   |43   |\n",
      "|2991       |2   |55   |\n",
      "+-----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "result.explain()\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898809",
   "metadata": {
    "id": "01898809"
   },
   "source": [
    "# Задание 3\n",
    "\n",
    "## Входные данные\n",
    "\n",
    "*cut_transactions.csv*  — информация о длительности просомтра контента пользователям\n",
    "\n",
    "Описание фичей в cut_transactions.csv:\n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "2.  element_uid — уникальный идентификатор контента\n",
    "3.  watched_time — время просмотра в секундах\n",
    "\n",
    "*cut_ratings.csv*  — информация об оценках, поставленных пользователями\n",
    "\n",
    "Описание фичей в cut_ratings.csv:\n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "2. element_uid — уникальный идентификатор контента\n",
    "3. rating — поставленный пользователем рейтинг\n",
    "\n",
    "*ids.csv*  — выборка пользователей\n",
    "Описание фичей в ids.csv:\n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "\n",
    "\n",
    "## Что нужно сделать\n",
    "Для каждого пользователя из выборки посчитать:\n",
    "1. Максимальное и минимальное время просмотра фильмов с оценками 8, 9 и 10\n",
    "2. Название фичи должно быть в формате feat_агрегирующая_функция_watched_time_rating_оценка.\n",
    "3. Если у пользователь не ставил оценки 8, 9 и 10 то значение фичей должно быть null\n",
    "4. Описать принятые при разработки кода решения и возможные оптимизации\n",
    "\n",
    "P.S. На каждом этапе обработки должно быть должны агрегироваться минимально возможные объемы данных (сокращаем затраты на shuflle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0963ed4",
   "metadata": {
    "id": "a0963ed4"
   },
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79070ec5-aa3c-4f74-b05e-b2238043f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/cut_transactions.csv\")\n",
    "ratings_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/cut_ratings.csv\")\n",
    "ids_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31dac489",
   "metadata": {
    "id": "31dac489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[element_uid: string, user_uid: string, watched_time: string],\n",
       " DataFrame[user_uid: string, element_uid: string, rating: string],\n",
       " DataFrame[user_uid: string])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df, ratings_df, ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40fc85fb",
   "metadata": {
    "id": "40fc85fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 20:07:47 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for left outer join.\n",
      "24/06/27 20:07:47 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined_data = transactions_df.join(ratings_df, [\"user_uid\", \"element_uid\"], \"inner\").join(F.broadcast(ids_df), \"user_uid\", \"inner\")\n",
    "\n",
    "aggregated_data = combined_data.groupBy(\"user_uid\").agg(\n",
    "    F.max(F.when(F.col(\"rating\") == 8, F.col(\"watched_time\"))).alias(\"max_watch_time_score_8\"),\n",
    "    F.min(F.when(F.col(\"rating\") == 8, F.col(\"watched_time\"))).alias(\"min_watch_time_score_8\"),\n",
    "    F.max(F.when(F.col(\"rating\") == 9, F.col(\"watched_time\"))).alias(\"max_watch_time_score_9\"),\n",
    "    F.min(F.when(F.col(\"rating\") == 9, F.col(\"watched_time\"))).alias(\"min_watch_time_score_9\"),\n",
    "    F.max(F.when(F.col(\"rating\") == 10, F.col(\"watched_time\"))).alias(\"max_watch_time_score_10\"),\n",
    "    F.min(F.when(F.col(\"rating\") == 10, F.col(\"watched_time\"))).alias(\"min_watch_time_score_10\")\n",
    ")\n",
    "\n",
    "result = F.broadcast(ids_df).join(aggregated_data, \"user_uid\", \"left\")\n",
    "\n",
    "result.write.csv(\"s3a://output/data/aggregated_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd40363a",
   "metadata": {
    "id": "bd40363a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 20:07:52 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for left outer join.\n",
      "24/06/27 20:07:52 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for left outer join.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(9) Project [user_uid#894, max_watch_time_score_8#912, min_watch_time_score_8#914, max_watch_time_score_9#916, min_watch_time_score_9#918, max_watch_time_score_10#920, min_watch_time_score_10#922]\n",
      "+- *(9) SortMergeJoin [user_uid#894], [user_uid#849], LeftOuter\n",
      "   :- *(1) Sort [user_uid#894 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(user_uid#894, 200), ENSURE_REQUIREMENTS, [plan_id=2166]\n",
      "   :     +- FileScan csv [user_uid#894] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/ids.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_uid:string>\n",
      "   +- SortAggregate(key=[user_uid#849], functions=[max(CASE WHEN (cast(rating#873 as int) = 8) THEN watched_time#850 END), min(CASE WHEN (cast(rating#873 as int) = 8) THEN watched_time#850 END), max(CASE WHEN (cast(rating#873 as int) = 9) THEN watched_time#850 END), min(CASE WHEN (cast(rating#873 as int) = 9) THEN watched_time#850 END), max(CASE WHEN (cast(rating#873 as int) = 10) THEN watched_time#850 END), min(CASE WHEN (cast(rating#873 as int) = 10) THEN watched_time#850 END)])\n",
      "      +- *(8) Sort [user_uid#849 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(user_uid#849, 200), ENSURE_REQUIREMENTS, [plan_id=2201]\n",
      "            +- SortAggregate(key=[user_uid#849], functions=[partial_max(CASE WHEN (cast(rating#873 as int) = 8) THEN watched_time#850 END), partial_min(CASE WHEN (cast(rating#873 as int) = 8) THEN watched_time#850 END), partial_max(CASE WHEN (cast(rating#873 as int) = 9) THEN watched_time#850 END), partial_min(CASE WHEN (cast(rating#873 as int) = 9) THEN watched_time#850 END), partial_max(CASE WHEN (cast(rating#873 as int) = 10) THEN watched_time#850 END), partial_min(CASE WHEN (cast(rating#873 as int) = 10) THEN watched_time#850 END)])\n",
      "               +- *(7) Project [user_uid#849, watched_time#850, rating#873]\n",
      "                  +- *(7) BroadcastHashJoin [user_uid#849], [user_uid#930], Inner, BuildRight, false\n",
      "                     :- *(7) Project [user_uid#849, watched_time#850, rating#873]\n",
      "                     :  +- *(7) SortMergeJoin [user_uid#849, element_uid#848], [user_uid#871, element_uid#872], Inner\n",
      "                     :     :- *(3) Sort [user_uid#849 ASC NULLS FIRST, element_uid#848 ASC NULLS FIRST], false, 0\n",
      "                     :     :  +- Exchange hashpartitioning(user_uid#849, element_uid#848, 200), ENSURE_REQUIREMENTS, [plan_id=2177]\n",
      "                     :     :     +- *(2) Filter (isnotnull(user_uid#849) AND isnotnull(element_uid#848))\n",
      "                     :     :        +- FileScan csv [element_uid#848,user_uid#849,watched_time#850] Batched: false, DataFilters: [isnotnull(user_uid#849), isnotnull(element_uid#848)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/cut_transactions.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_uid), IsNotNull(element_uid)], ReadSchema: struct<element_uid:string,user_uid:string,watched_time:string>\n",
      "                     :     +- *(5) Sort [user_uid#871 ASC NULLS FIRST, element_uid#872 ASC NULLS FIRST], false, 0\n",
      "                     :        +- Exchange hashpartitioning(user_uid#871, element_uid#872, 200), ENSURE_REQUIREMENTS, [plan_id=2185]\n",
      "                     :           +- *(4) Filter (isnotnull(user_uid#871) AND isnotnull(element_uid#872))\n",
      "                     :              +- FileScan csv [user_uid#871,element_uid#872,rating#873] Batched: false, DataFilters: [isnotnull(user_uid#871), isnotnull(element_uid#872)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/cut_ratings.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_uid), IsNotNull(element_uid)], ReadSchema: struct<user_uid:string,element_uid:string,rating:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2195]\n",
      "                        +- *(6) Filter isnotnull(user_uid#930)\n",
      "                           +- FileScan csv [user_uid#930] Batched: false, DataFilters: [isnotnull(user_uid#930)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/ids.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_uid)], ReadSchema: struct<user_uid:string>\n",
      "\n",
      "\n",
      "+--------+----------------------+----------------------+----------------------+----------------------+-----------------------+-----------------------+\n",
      "|user_uid|max_watch_time_score_8|min_watch_time_score_8|max_watch_time_score_9|min_watch_time_score_9|max_watch_time_score_10|min_watch_time_score_10|\n",
      "+--------+----------------------+----------------------+----------------------+----------------------+-----------------------+-----------------------+\n",
      "|524044  |NULL                  |NULL                  |NULL                  |NULL                  |7743                   |7547                   |\n",
      "|436138  |NULL                  |NULL                  |NULL                  |NULL                  |7652                   |7652                   |\n",
      "|241442  |NULL                  |NULL                  |NULL                  |NULL                  |99265                  |99265                  |\n",
      "|488504  |NULL                  |NULL                  |NULL                  |NULL                  |14411                  |14411                  |\n",
      "|126454  |NULL                  |NULL                  |NULL                  |NULL                  |34076                  |34076                  |\n",
      "|74017   |NULL                  |NULL                  |NULL                  |NULL                  |6001                   |6001                   |\n",
      "|460484  |NULL                  |NULL                  |NULL                  |NULL                  |6959                   |6959                   |\n",
      "|131626  |NULL                  |NULL                  |NULL                  |NULL                  |7205                   |7205                   |\n",
      "|6727    |NULL                  |NULL                  |NULL                  |NULL                  |7593                   |7593                   |\n",
      "|355521  |NULL                  |NULL                  |NULL                  |NULL                  |7210                   |7210                   |\n",
      "|207861  |NULL                  |NULL                  |NULL                  |NULL                  |8088                   |8088                   |\n",
      "|569885  |NULL                  |NULL                  |NULL                  |NULL                  |7414                   |7414                   |\n",
      "|145083  |NULL                  |NULL                  |NULL                  |NULL                  |7361                   |7361                   |\n",
      "|315498  |NULL                  |NULL                  |NULL                  |NULL                  |7469                   |7469                   |\n",
      "|489751  |NULL                  |NULL                  |NULL                  |NULL                  |5447                   |5447                   |\n",
      "|88783   |NULL                  |NULL                  |NULL                  |NULL                  |14401                  |14401                  |\n",
      "|460607  |NULL                  |NULL                  |NULL                  |NULL                  |5245                   |5245                   |\n",
      "|25048   |NULL                  |NULL                  |NULL                  |NULL                  |8466                   |6778                   |\n",
      "|379969  |NULL                  |NULL                  |NULL                  |NULL                  |7731                   |13884                  |\n",
      "|394262  |NULL                  |NULL                  |NULL                  |NULL                  |5717                   |5717                   |\n",
      "+--------+----------------------+----------------------+----------------------+----------------------+-----------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "result.explain()\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f8a8e-f21f-4499-aae4-1ed1b0e5027d",
   "metadata": {},
   "source": [
    "Был использован **broadcast has join**, так как таблицы ids.csv и cut_ratings.csv небольшие (т.к. < 10 мб). "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
